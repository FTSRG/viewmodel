---
title: "Data Analysis for \"Incremental View Model Synchronization Using Partial Models\""
author: "Kristóf Marussy, Oszkár Semeráth, Dániel Varró"
date: "July 10, 2018"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This report contains data analysis for the papaer "Incremental View Model Synchronization Using Partial Models".

The source code of the ViewModel tool is available under the terms of the Eclipse Public License, version 1.0 at https://github.com/FTSRG/viewmodel.

# Evaluation

## Research questions

Our view transformation approach is fully implemented as an [open source project](https://github.com/FTSRG/viewmodel). We carried out an experimental evaluation to address three research questions: 

**RQ1.** What is the complexity of different execution phases in our view transformation engine?

**RQ2.** What is the performance overhead for the initial run of our view transformation engine compared to reactive imperative transformations with explicit traceability?

**RQ3.** What is the performance overhead for change-driven behavior of our view transformation engine compared to reactive imperative transformations with explicit traceability?

## Case studies

We selected two substantially different view transformation challenges for our investigation.

1. *VirtualSwitch* is a filtering transformation taken from the VAO'14 paper [Query-driven incremental synchronization of view models](https://dl.acm.org/citation.cfm?id=2631677) (see the [publication page](https://github.com/FTSRG/publication-pages/wiki/Query-driven-synchronization) for details). The implementation of this case study is available in the [ViewModel repository](https://github.com/FTSRG/viewmodel/blob/models2018/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.viewmodel/src/hu/bme/mit/inf/viewmodel/benchmarks/viewmodel/virtualswitchview/RailwayModel2VirtualSwitchModel.viewmodel).

2. *Dependability* is an extended version of the case study used in this paper which aims to compose two separate transformations in a way that the target Petri net model is significantly larger than any of the two source models. The implementation of this case study is available in the [ViewModel repository](https://github.com/FTSRG/viewmodel/blob/models2018/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.viewmodel/src/hu/bme/mit/inf/viewmodel/benchmarks/viewmodel/dependability/Dependability2PetriNet.viewmodel).

We believe that these transformations are representative for key practical applications of view transformations: the *VirtualSwitch* scenario is typical for in traditional view models with information loss while the transformation challenges in the *Dependability* case are common for the formal analysis of extra-functional properties of systems.

## Compared approaches

First, we instrumented our *ViewModel* transformation approach to enable the clear separation of different transformation phases to address **RQ1.** Then we compare our approach with two different view transformation styles available in VIATRA. These solutions use an *explicit traceability model* (vs. implicit traceability in our approach) and *imperative actions* in transformation rules using Java/Xtend (vs. declarative query-based templates). However, differences in query performance can be mitigated to a large extent.
(1.a) The [*source-reactive* solution](https://github.com/FTSRG/viewmodel/blob/models2018/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.viatra.driver/src/hu/bme/mit/inf/viewmodel/benchmarks/viatra/transformation/DependabilityModel2StochasticPetriNetPrioritised.xtend) uses exactly the same source queries as our view transformation approach, but rule priorities had to be set carefully. (1.b) The [*trace-reactive* solution](https://github.com/FTSRG/viewmodel/blob/models2018/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.viatra.driver/src/hu/bme/mit/inf/viewmodel/benchmarks/viatra/transformation/DependabilityModel2StochasticPetriNet.xtend) uses queries with both source and traceability elements as part of its precondition. Since both the level of compositionality and the properties of the view transformation engine are different in these approaches compared to our view transformation approach, our evaluation may reveal the performance trade-offs of the increased expressiveness of our approach.

Our repository contains an implementation of the transformations in [batch ATL](https://github.com/FTSRG/viewmodel/tree/models2018/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.atl/transformation) and a partial implementation in [eMoflon](https://github.com/FTSRG/viewmodel/tree/models2018/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.tgg.stochasticpetrinet), but the different performance optimizations in those tools would disallow to separate query performance from transformation performance.

## Experiment setup

To investigate the initial transformation runs **RQ2**, our measurement setup contains 5 source models of increasing size. For the *VirtualSwitch* case, the source models were ranging from 25K to 425K elements, while the target models were ranging from 500 to 9K elements. For the *Dependability* case, the source models ranged 1K to 25K while the target models ranged from 3K to 72K. In each case, we measured the initial time for populating the caches of queries and the execution time of the first transformation, while the load time of source models was excluded. To address **RQ1**, we measure how much time the different phases of our view transformation approach takes during this initial run.

To investigate the change-driven behavior (**RQ3**), we first created 10 different elementary changes (modifications of one element) and 5 change mixes containing 100 elementary changes each (with fix ratio between different types of change within each mix).

Change mix (A) presents a balanced mix of changes, while types of changes in mixes (B) and (C) were selected from those elementary changes that caused longer synchronization times in the *Dependability* and *VirtualSwitch* cases, respectively. Mix (D) were selected from changes that caused longer synchronization times in at least one of cases as a compromise between B and C, where changes in (E) caused longer synchronization times in neither.

| Elementary change | Mix (A) | Mix (B) | Mix (C) | Mix (D) | Mix (E) |
|:------------------|--------:|--------:|--------:|--------:|--------:|
| Create switch | 10 | 25 | 0 | 12 | 0 |
| Create segment | 10 | 0 | 0 | 0 | 50 |
| Connect track elements | 10 | 0 | 0 | 0 | 50 |
| Disconnect track elements | 10 | 0 | 33 | 16 | 0 |
| Create route | 5 | 25 | 0 | 12 | 0 |
| Remove route | 5 | 5 | 0 | 5 | 0 |
| Add switch to route | 10 | 20 | 0 | 10 | 0 |
| Remove switch from route | 10 | 25 | 0 | 12 | 0 |
| Set switch failed | 15 | 0 | 34 | 17 | 0 |
| Set switch operational | 15 | 0 | 33 | 16 | 0 |

Each experiment was executed 30 times after 10 warmup runs on a cloud-based virtual environment (with 4 CPU, 16 GB memory and 8 GB disk size, `m5.xlarge`) on Amazon AWS. Heap space of the Java virtual machine was limited to 15 GB.


# Loading the data

First we load the `tidyverse` packages for data wrangling and plotting.

```{r, message=FALSE}
require(tidyverse)
```

The file `full_log.csv` is the concatenation of the log files produced by the benchmark configurations `d_batch.json`, `d_1.json`, `d_2.json`, ..., `d_15.json`, as well as `vs_batch.json`, `vs_1.json`, `vs_2.json`, ..., `vs_15.json`.

```{r, message=FALSE}
log_path <- './full_log.csv'
full_log <- read_csv(log_path, col_types = cols(
  model = col_character(),
  transformationCase = col_character(),
  experiment = col_character(),
  modificationMix = col_character(),
  rerun = col_integer(),
  variable = col_character(),
  value = col_double()
))
```

We only measured using Train Benchmark models, so we can replace the model name with the scale factor in the logs, while still preserving all information.

```{r}
trainbenchmark_log <- full_log %>%
  mutate(modelSize = as.integer(gsub('railway-batch-', '', model))) %>%
  select(-model) %>%
  separate(variable, c('checkpoint', 'category', 'variable'))
```

# Basic statistics

We define some helper function for converting string identifiers to factor variables later.

```{r}
TransformationCaseFactor <- function (v) {
  factor(as.factor(v),
         levels = c('dependability', 'virtualSwitch'),
         labels = c('Dependability', 'VirtualSwitch'))
}

ModificationMixFactor <- function (v) {
  factor(as.factor(v),
         levels = c('modelQuery', 'execute', 'usual', 'petriNetSlow', 'virtualSwitchSlow',
                    'bothSlow', 'bothFast', 'createSwitch', 'createSegment',
                    'connectTrackElements', 'disconnectTrackElements', 'createRoute', 'removeRoute',
                    'addSwitchToRoute', 'removeSwitchFromRoute',
                    'setSwitchFailed', 'setSwitchOperational'),
         labels = c('Initial query', 'Initial transformation', '(A) Usual mix', '(B) Depend. stress mix',
                    '(C) VirtSw. stress mix', '(D) mix', '(E) mix', 'Create switch', 'Create segment',
                    'Connect tract elements', 'Disconnect track elements', 'Create route', 'Remove route',
                    'Add switch to route', 'Remove switch from route',
                    'Set switch failed', 'Set switch operational'))
}

ExperimentFactor <- function (v) {
  factor(as.factor(v),
         levels = c('viewModel-physical', 'viatra-priorities', 'viatra'),
         labels = c('Our approach', 'Source-reactive VIATRA', 'Trace-reactive VIATRA'))
}
```

## Source model statistics

We create a data frame containing size of the source models (object, reference and attribute counts). Each run prints the model sizes. However, we only use the output from the first batch run, because all source models with the same scale factor are identical.

```{r, results='asis'}
source_model_statistics <- trainbenchmark_log %>%
  filter(experiment == 'viewModel-batch-physical' &
           modificationMix == 'none' &
           rerun == 0 &
           checkpoint == 'batch' &
           category == 'source') %>%
  group_by(modelSize, variable) %>%
  summarize(value = first(value)) %>%
  mutate(variable = paste0('source_', variable)) %>%
  spread(variable, value)

stat_df <- data.frame(source_model_statistics$modelSize, source_model_statistics$source_count, source_model_statistics$source_referenceCount, source_model_statistics$source_attributeCount)

colnames(stat_df) <- c("Scale factor", "Source objects", "Source references", "Source attributes")
knitr::kable(stat_df, format='markdown') %>% cat(sep = '\n')
```

## Target model statistics

### Batch transformations

We perform the same analysis for the target models of the batch transformations.

```{r, results='asis'}
target_model_statistics <- trainbenchmark_log %>%
  filter(experiment == 'viewModel-batch-physical' &
           modificationMix == 'none' &
           rerun == 0 &
           checkpoint == 'batch' &
           category == 'target') %>%
  group_by(modelSize, transformationCase, variable) %>%
  summarize(value = first(value)) %>%
  mutate(variable = paste0('target_', variable)) %>%
  spread(variable, value)

tstat_cases <- TransformationCaseFactor(target_model_statistics$transformationCase)
tstat_df <- data.frame(target_model_statistics$modelSize, tstat_cases, target_model_statistics$target_count, target_model_statistics$target_referenceCount, target_model_statistics$target_attributeCount)

colnames(tstat_df) <- c("Scale factor", "Case study", "Target objects", "Target references", "Target attributes")
knitr::kable(tstat_df, format='markdown') %>% cat(sep = '\n')
```

For sanity, we check that all transformations resulted in the same number of target model elements.

```{r}
bad_batch_transformations <- trainbenchmark_log %>%
  filter(modificationMix == 'none' &
           checkpoint == 'batch' &
           category == 'target') %>%
  select(-c(modificationMix, checkpoint, category)) %>%
  mutate(variable = paste0('actual_', variable)) %>%
  spread(variable, value) %>%
  inner_join(target_model_statistics, by=c('transformationCase', 'modelSize')) %>%
  filter(actual_rootCount != target_rootCount |
           actual_count != target_count |
           actual_referenceCount != target_referenceCount |
           actual_attributeCount != target_attributeCount)

if (nrow(bad_batch_transformations) != 0) {
  print(bad_batch_transformations)
  stop("Unexpected batch transformation results")
} else {
  message("All correct")
}
```

We may see that every batch transformation resulted in the expected number of target elements.

Now we count the variables and constraints in the partial models.

```{r, results='asis'}
partial_size <- trainbenchmark_log %>%
  filter(modificationMix == 'none' &
           checkpoint == 'batch' & category == 'trace' &
           variable %in% c('variableCount', 'constraintCount') &
           rerun == 0) %>%
  select(c(transformationCase, variable, value, modelSize)) %>%
  spread(variable, value)

partial_cases <- TransformationCaseFactor(partial_size$transformationCase)
partial_df = data.frame(partial_size$modelSize, partial_cases, partial_size$variableCount, partial_size$constraintCount)

colnames(partial_df) <- c("Scale factor", "Case study", "Partial model variables", "Partial model constraints")
knitr::kable(partial_df, format='markdown') %>% cat(sep = '\n')
```

### Change-driven transformations

Different modification mixed produce different output models, so we also collect statistics for the target model after each modification mix separately.

```{r, results='asis'}
incremental_target_model_statistics <- trainbenchmark_log %>%
  filter(experiment == 'viewModel-incremental-physical' &
           modificationMix != 'none' &
           rerun == 0 &
           checkpoint == 'after' &
           category == 'target') %>%
  group_by(modelSize, transformationCase, modificationMix, variable) %>%
  summarize(value = first(value)) %>%
  mutate(variable = paste0('target_', variable)) %>%
  spread(variable, value) %>%
  ungroup()

incremental_tstat_df <- data.frame(
  incremental_target_model_statistics$modelSize,
  TransformationCaseFactor(incremental_target_model_statistics$transformationCase),
  ModificationMixFactor(incremental_target_model_statistics$modificationMix),
  incremental_target_model_statistics$target_count,
  incremental_target_model_statistics$target_referenceCount,
  incremental_target_model_statistics$target_attributeCount) %>%
  arrange_at(c(2, 1, 3))

colnames(incremental_tstat_df) <- c("Scale factor", "Case study", "Modification mix", "Target objects", "Target references", "Target attributes")
knitr::kable(incremental_tstat_df, format='markdown') %>% cat(sep = '\n')
```

We make sure that each experiment resulted in the same number of target model elements when executed with the same source model and modification mix.

```{r}
incremental_target_model_statistics_by_experiment <- trainbenchmark_log %>%
  filter(modificationMix != 'none' &
           checkpoint == 'after' &
           category == 'target') %>%
  group_by(modelSize, transformationCase, modificationMix, experiment, rerun, variable) %>%
  summarize(value = first(value)) %>%
  mutate(variable = paste0('actual_', variable)) %>%
  spread(variable, value) %>%
  ungroup()

bad_incremental_experiments <- incremental_target_model_statistics_by_experiment %>%
  inner_join(incremental_target_model_statistics,
    by = c('modelSize', 'transformationCase', 'modificationMix')) %>%
  filter(actual_rootCount != target_rootCount |
           actual_count != target_count |
           actual_referenceCount != target_referenceCount |
           actual_attributeCount != target_attributeCount)


if (nrow(bad_incremental_experiments) != 0) {
  print(bad_incremental_experiments)
  stop("Unexpected incremental transformation results")
} else {
  message("All correct")
}
```

We may see that every incremental transformation resulted in the expected number of target elements.

# Execution time (RQ2 and RQ3)

## Data wrangling

We define a helper function for collecting execution times from the batch and incremental versions of experiments.

```{r}
RenameExperiment <- function(df) {
  df %>% mutate(experiment = gsub("-(batch|incremental)", "", experiment))
}
```

### First execution (RQ2)

We will "formally" treat model query and first execution as two modification mixes, which simplifies our data frames.
They will be shown on the same plots as the incremental sychronization times, anyway.

We collect the execution time of the model query in the first (also known as "batch" in the logs) execution.
This is relatively simple, as all experiments contain a `modelQuery` checkpoint.

```{r}
modelQuery <- trainbenchmark_log %>%
  filter(modificationMix == 'none' &
           variable == 'duration' &
           checkpoint == 'modelQuery') %>%
  mutate(modificationMix='modelQuery') %>%
  RenameExperiment()
```

We extract the first execution time of the hand-written VIATRA transformations, too.
Only the hand-written transformations have an `execute` checkpoint in the log.

```{r}
execution_viatra <- trainbenchmark_log %>%
  filter(modificationMix == 'none' &
           variable == 'duration' &
           checkpoint == 'execute') %>%
  mutate(modificationMix='execute') %>%
  RenameExperiment()
```

Extracting the first run of the ViewModel transformations is a bit more involved, as different steps were logged separately.
We simply sum their durations.

```{r}
execution_viewmodel <- trainbenchmark_log %>%
  filter(modificationMix == 'none' &
           variable == 'duration' &
           checkpoint %in% c('pt2tExecute', 'pt2tRete', 's2ptExecute', 's2ptRete')) %>%
  group_by(transformationCase, experiment, rerun, category, variable, modelSize) %>%
  summarize(checkpoint='execute', modificationMix='execute', value=sum(value)) %>%
  ungroup() %>%
  RenameExperiment()
```

### Incremental transformations (RQ3)

The execution time of change-driven synchronization is split between the `modelModification` (propagation of source model changes trough the RETE net) and `synchronization` (firing of change-driven transformation rules) phases.
We sum the two durations.

```{r}
incremental <- trainbenchmark_log %>%
  filter(modificationMix != 'none' &
           variable == 'duration' &
           checkpoint %in% c('synchronization', 'modelModification')) %>%
  group_by(transformationCase, experiment, rerun, category, variable, modelSize, modificationMix) %>%
  summarize(checkpoint='synchronization', value=sum(value)) %>%
  ungroup() %>%
  RenameExperiment()
```

### Putting it together

We bind the data frames from the previous two sections, and join them to the target and source model statistics.
Then we prepare for creating the plots as follows:

  * The total model size is the geometric mean of the number of source and target model objects.
    This measure was selected because it can indicate scaleability in both case studies considered.
  * Only the median of the execution times is kept.
  * Execution times smaller than 1 ms are replaced with 1 ms so that a log-log plot can be drawn.
    Zero execution times would lead to `NaN` values after taking their logarithm.

```{r}
durations_plot <- rbind(incremental, modelQuery, execution_viatra, execution_viewmodel) %>%
  inner_join(target_model_statistics, by = c('transformationCase', 'modelSize')) %>%
  inner_join(source_model_statistics, by = c('modelSize')) %>%
  mutate(total_count = sqrt(source_count * target_count)) %>%
  group_by(modelSize, transformationCase, modificationMix, experiment) %>%
  summarize(total_count = median(total_count), value = median(value)) %>%
  mutate(value = ifelse(value < 1, 1, value))
```

We add some factor labels in order to generate appropriate legends in the plots.

```{r}
durations_plot$modificationMix <- ModificationMixFactor(durations_plot$modificationMix)
durations_plot$transformationCase <- TransformationCaseFactor(durations_plot$transformationCase)
durations_plot$experiment <- ExperimentFactor(durations_plot$experiment)
durations_plot <- durations_plot %>%
  arrange(transformationCase, modelSize, modificationMix, experiment)
```

A large table can be assembled for viewing by spreading the three experiments side by side.
Note that different modification mixes were evaluated on different virtual machines, so executing times are only comparable within a single modification mix.

```{r, results='asis'}
durations_table <- durations_plot %>%
  spread(experiment, value) %>%
  arrange(modificationMix, transformationCase, modelSize)
colnames(durations_table)[1:4] <- c("Scale factor", "Case study", "Modification mix", "Total size")
knitr::kable(durations_table, format='markdown') %>% cat(sep = '\n')
```

## Plots

Let use define some helper functions for making publication-quality plots.

```{r}
scientific_10 <- function (x) {
  parse(text=gsub("1e", " 10^", scales::scientific_format()(x)))
}

DurationsPlot <- function (df) {
  ggplot(df, aes(x = total_count, y = value, color=experiment, shape=experiment)) +
    geom_point(size = 2) +
    geom_line() +
    scale_x_continuous(name = "Model size = sqrt(#source objects * #target object)",
                       trans = "log",
                       limits = c(1000, 100000),
                       breaks = c(1, 10, 100, 1000, 10000, 100000),
                       label=scientific_10) +
    scale_y_continuous(name = "Execution time (ms)",
                       trans = 'log',
                       limits = c(1, 150000),
                       breaks = c(1, 10, 100, 1000, 10000, 100000),
                       label=scientific_10) +
    facet_grid(transformationCase~modificationMix) +
    scale_color_brewer(type='qual', palette=6, name = "Transformation") +
    scale_shape_manual(values=c(1, 4, 3), name="Transformation") +
    theme_bw() +
    theme(legend.position='bottom', legend.box.spacing = unit(c(0, 0, 0, 0), 'cm'))
}
```

We split the plots into three blocks due to their size.

```{r, fig.height=5,fig.width=12}
durations_plot %>% filter(as.numeric(modificationMix) < 8) %>% DurationsPlot()
```

```{r, fig.height=5,fig.width=9}
durations_plot %>% filter(as.numeric(modificationMix) >= 8 & as.numeric(modificationMix) < 13) %>% DurationsPlot()
```

```{r, fig.height=5,fig.width=9}
durations_plot %>% filter(as.numeric(modificationMix) >= 13) %>% DurationsPlot()
```

# Instrumented first-run transformations (RQ1)

In order to analyze first run behavior in depth, ViewModel was instrumented to log each phase of first run execution separately.
These phases are

  * constructing the Source2PartialTarget RETE net,
  * firing the Source2PartialTarget transformation rules,
  * constructing the PartialTarget2Target RETE net,
  * firing the PartialTarget2Target transformation rules.

As the RETE constrution times are usually much shorter than the firing time, we add them to the firing times and compare the overall execution times of the Source2PartialTarget and PartialTarget2Target phases.

```{r}
instrumented_plot <- trainbenchmark_log %>%
  filter(modificationMix == 'none' &
           variable == 'duration' &
           checkpoint %in% c('pt2tExecute', 'pt2tRete', 's2ptExecute', 's2ptRete')) %>%
  mutate(checkpoint = gsub('Execute|Rete', '', checkpoint)) %>%
  group_by(transformationCase, experiment, rerun, category, variable, modelSize, checkpoint, modificationMix) %>%
  summarize(value=sum(value)) %>%
  ungroup() %>%
  inner_join(target_model_statistics, by = c('transformationCase', 'modelSize')) %>%
  inner_join(source_model_statistics, by = c('modelSize')) %>%
  mutate(total_count = sqrt(source_count * target_count)) %>%
  group_by(modelSize, transformationCase, modificationMix, experiment, checkpoint) %>%
  summarize(total_count = median(total_count), value = median(value)) %>%
  mutate(value = ifelse(value < 1, 1, value)) %>%
  ungroup()

instrumented_plot$transformationCase <- TransformationCaseFactor(instrumented_plot$transformationCase)
instrumented_plot$checkpoint <- factor(as.factor(instrumented_plot$checkpoint),
                                    levels = c('s2pt', 'pt2t'),
                                    labels = c('S2PT', 'PT2T'))
```

```{r, results='asis'}
show_instrumented_plot <- instrumented_plot %>%
  select(-c('modificationMix', 'experiment')) %>%
  spread(checkpoint, value)
colnames(show_instrumented_plot) <- c('Scale factor', 'Case study', 'Total size', 'PT2T duration', 'S2PT duration')

knitr::kable(show_instrumented_plot, format='markdown') %>% cat(sep = '\n')
```

```{r, fig.height=5,fig.width=3}
ggplot(instrumented_plot, aes(x = total_count, y = value, color=checkpoint, shape=checkpoint)) +
  geom_point(size=3.5) +
  geom_line() +
  scale_x_continuous(name = "Model size",
                     trans = "log",
                     limits = c(1000, 100000),
                     breaks = c(1, 10, 100, 1000, 10000, 100000),
                     label=scientific_10) +
  scale_y_continuous(name = "Execution time (ms)",
                     trans = 'log',
                     limits = c(1, 150000),
                     breaks = c(1, 10, 100, 1000, 10000, 100000),
                     label=scientific_10) +
  facet_grid(transformationCase~.) +
  scale_color_brewer(type='qual', palette=6, name = "Execution step") +
  scale_shape_manual(values=c(1, 4), name="Execution step") +
  theme_bw() +
  theme(legend.position='bottom', legend.box.spacing = unit(c(0, 0, 0, 0), 'cm'))
```
